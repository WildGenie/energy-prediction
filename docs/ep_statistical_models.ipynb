{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "energy_predictor_2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_aU9e5WJwaW",
        "colab_type": "text"
      },
      "source": [
        "Part 2 of **time series forecasting with energy**\n",
        "\n",
        "In this section, I will investigate the problem at hand, formalising it, outlining methods for interpreting error, and setup a baseline using standard statistical methods for time-series analysis. The library `statsmodels` will come in handy with this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBUkdRBIziF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD THE REPOSITORY\n",
        "# if you are working from outside the repository\n",
        "# this happens if you use colab like I do\n",
        "!git clone https://github.com/sandeshbhatjr/energy-prediction.git\n",
        "!pip install -U --quiet pandas statsmodels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRXzaf6uM2NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hN3GD0GIe9K",
        "colab_type": "text"
      },
      "source": [
        "# Predictive models\n",
        "\n",
        "This part will be technical. The purpose is to explore SOTA models for time-series forecasting, and their applicability to day-ahead price forecasting. In a typical use-case, we have access to a history of recorded day-ahead prices, and we are interested in forecasting the day-ahead prices in an interval consisting of a tuple of 24 prices for each day. The problem can be formulated precisely as follows.\n",
        ">  Let $\\{t_1, t_2, t_3, ..., t_k \\} := \\mathfrak{T}$ represent a collection of dates in  chronological order, $p_t \\in \\mathbb{R}^{24}$ be the day-ahead price indexed by $t \\in \\mathfrak{T}$. Given some historic day-ahead prices $p_{t_1}, p_{t_2}, ... ,p_{t_k}$, forecast the future day-ahead prices $p_{t_{k+1}}, ..., p_{t_{k+h}}$ in a certain future window-size of $h \\in \\mathbb{N}$. In addition, we have a collection of exogenous time series- $d^{\\alpha}_{t_1},d^{\\alpha}_{t_2}, ..., d^{\\alpha}_{t_k}$ indexed by $\\alpha$, which includes the generation and consumption forecasts. Finally for each $t\\in {t_1, ..., t_{k+h'}}, h'>> h$, we have some associated features $f^{\\beta}_t$, where the features are known well into future (for example, if the day is a holiday or not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8j275Z-IjbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractHourlyData(df, data_columns, hour_column='Hour', date_column='Date'):\n",
        "  num_of_sample = len(df['Date'].unique())\n",
        "  num_of_data_columns = len(data_columns)\n",
        "  hourly_data = np.zeros((num_of_sample, 24*num_of_data_columns))\n",
        "  for i, data_column in enumerate(data_columns):\n",
        "    for d in range(24):\n",
        "      hourly_data[:, i*24 + d] = df[df[hour_column] == d].sort_values(by=date_column).loc[:, data_column]\n",
        "  return hourly_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NuEz3EKOTV",
        "colab_type": "text"
      },
      "source": [
        "**Windowed dataset using Keras Timeseries generator**\n",
        "\n",
        "Most models will work not on the entire time-series, but on a windowed subset of the time series. Keras has an inbuilt windowed dataset generator, but it helps to know a few things before we use that. It is primarily meant for use with Keras, so it produces a generator that outputs the dataset in batches as a list $[b_1, b_2, ...]$. Each batch $b_i$ consists of $[X,y]$, with each being a numpy array. The format here is as follows: each $X$ is a windowed dataset with following indices: `(sample_num, timestep, feature)`, while $y$ is of the form `(sample_num, value)`. The assumption here is that `y` consists of a single timestep, which works fine in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIeAIoDKOw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "def get_windowed_dataset(df, X_cols, y_cols, window_size, return_generator=False, **kwargs):\n",
        "  X_data = extractHourlyData(df, X_cols)\n",
        "  y_data = extractHourlyData(df, y_cols)\n",
        "  # use batch size if defined, else return the whole dataset in one batch\n",
        "  batch_size = kwargs['batch_size'] or len(X_data)\n",
        "  generator = TimeseriesGenerator(\n",
        "    X_data, \n",
        "    y_data, \n",
        "    length = window_size, \n",
        "    sampling_rate = 1,\n",
        "    batch_size = batch_size)\n",
        "  if return_generator:\n",
        "    return generator\n",
        "  else:\n",
        "    X_batches = [X for (X, y) in generator]\n",
        "    y_batches = [y for (X, y) in generator]\n",
        "    return X_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrUDKYQGTceQ",
        "colab_type": "text"
      },
      "source": [
        "Just a quick check to see if the generator works as expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4U3qYrTbzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = get_windowed_dataset(\n",
        "    german_df, \n",
        "    ['Day Ahead Price'], \n",
        "    ['Day Ahead Price'], \n",
        "    1, \n",
        "    return_generator=True, \n",
        "    batch_size=len(german_df)\n",
        ")\n",
        "\n",
        "generator[0][0].shape, generator[0][1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9NgiOPfTfxL",
        "colab_type": "text"
      },
      "source": [
        "Before proceeding to investigate our models, it is important to figure out a cross-validation strategy and testing strategy using a metric to assess the performance of each of our models. So, let us briefly touch upon these topics first.  \n",
        "\n",
        "1. The cross-validation strategy for temporal data is a bit different from randomly choosing a subset of the dataset. Since the goal of our analysis is to forecast future values using present data, we will likewise train on a dataset from the past, and validate it against a subset to the future of it. This simulates the actual situation at hand; see [3, 4] for more details.\n",
        "2. For a simple criterion to do a quick comparison of algorithms as a first step in model evaluation, the sMAPE suffices:\n",
        "$$s = \\frac{|y-\\hat{y}|}{2(|y| + |\\hat{y}|)} $$ This should be sufficient for a first-order estimate of how well our models are doing; later on, we can perform more sophisticated forms of tests for our analysis.\n",
        "\n",
        "To cover pt. 1, we create a function to extract dataset into a train and test form chronologically separated as described. We also create a GridsearchCV equivalent for time series for hyperparameter search in ML models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5vGQvcUKmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_train_timesplit(df, date_col_name='Date', train_size=0.9, test_size=0.1):\n",
        "  \"\"\"\n",
        "    Returns test-train split data based on date with test chronologically later than train data.\n",
        "  \"\"\"\n",
        "  min_date = df[date_col_name].min()\n",
        "  max_date = df[date_col_name].max()\n",
        "  train_split_date = min_date + (train_size*(max_date - min_date))\n",
        "  test_split_date = train_split_date + (test_size*(max_date - min_date))\n",
        "  train_df = df[df[date_col_name] < train_split_date]\n",
        "  test_df = df[(df[date_col_name] > train_split_date) & (df[date_col_name] < test_split_date)]\n",
        "  return train_df, test_df\n",
        "\n",
        "def day_forward_chaining(df, date_col_name='Date', k=10):\n",
        "  for i in range(1,k):\n",
        "    yield test_train_timesplit(df, date_col_name, train_size=(i/k), test_size=(1/k))\n",
        "\n",
        "# The above function is the time-series equivalent of gridsearch CV\n",
        "# Example use-case:\n",
        "# for train_df, test_df in day_forward_chaining(german_df):\n",
        "#   <<< do your model training and tuning here >>>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_afRkMM-Cl",
        "colab_type": "text"
      },
      "source": [
        "# Naive model and the baseline\n",
        "\n",
        "The simplest model one can think of is as follows: $p_{t+h} = p_t $ for all $h$ in the future window. This should allow us to setup a baseline, which any sensible model should perform better than. This is primarily helpful in debugging complex models when making sure that we are not doing anything terribly stupid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAQg6j9NmN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxKxylBJN9mc",
        "colab_type": "text"
      },
      "source": [
        "# Regression\n",
        "\n",
        "For time-series data, regression usually presents itself in the form of auto-regression, i.e. regression in terms of previous time-step variable. A very simple example would be the $AR(k)$ models, where the hypothesis is as follows:\n",
        "$$p_{t+h}=\\sum_{i=0}\\alpha_ip_{t-i}+\\epsilon$$ where $\\epsilon$ is random white noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUYzrgu_N_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8K_FDnWRyu",
        "colab_type": "text"
      },
      "source": [
        "The above technique is a very simple one; there are far more sophistical statistical techniques available. We will pursue them here, but will use the `statsmodels` library. I will not explore the algorithms itself in detail (though it will make for a good series of blog posts sometime in the near future; for now refer to [1] for details)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8bl3eYi4cx",
        "colab_type": "text"
      },
      "source": [
        "## Univariate AR(k) with time trend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfxAmgJiq4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
        "from statsmodels.tsa.api import acf, pacf, graphics\n",
        "\n",
        "AR_model = AutoReg(german_df, 3, trend='t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQoWhaZn6n_",
        "colab_type": "text"
      },
      "source": [
        "## Multivariate VAR(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U4VJOsxO6Nw",
        "colab_type": "text"
      },
      "source": [
        "# Exponential Smoothing and Holtz-Winter models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWw8aLSsO9b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4twMe_WHm0",
        "colab_type": "text"
      },
      "source": [
        "# Markov switching regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDQ3w4wWNXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woxv6ti6kMbM",
        "colab_type": "text"
      },
      "source": [
        "# Bibliography\n",
        "\n",
        "**[1]**. R. J. Hyndman, and G. Athanasopoulos, Forecasting: Principles and Practice, https://otexts.com/fpp2/index.html "
      ]
    }
  ]
}