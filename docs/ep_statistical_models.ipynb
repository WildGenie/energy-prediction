{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ep_statistical_models",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_aU9e5WJwaW",
        "colab_type": "text"
      },
      "source": [
        "Part 2 of **time series forecasting with energy**\n",
        "\n",
        "In this section, I will investigate the problem at hand, formalising it, outlining methods for interpreting error, and setup a baseline using standard statistical methods for time-series analysis. The library `statsmodels` will come in handy with this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBUkdRBIziF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD THE REPOSITORY\n",
        "# if you are working from outside the repository\n",
        "# this happens if you use colab like I do\n",
        "!git clone https://github.com/sandeshbhatjr/energy-prediction.git\n",
        "!pip install -U --quiet pandas statsmodels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRXzaf6uM2NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hN3GD0GIe9K",
        "colab_type": "text"
      },
      "source": [
        "# Statistical models\n",
        "\n",
        "This part will be technical. The purpose is to explore SOTA models for time-series forecasting, and their applicability to day-ahead price forecasting. In a typical use-case, we have access to a history of recorded day-ahead prices, and we are interested in forecasting the day-ahead prices in an interval consisting of a tuple of 24 prices for each day. The problem can be formulated precisely as follows.\n",
        ">  Let $\\{t_1, t_2, t_3, ..., t_k \\} := \\mathfrak{T}$ represent a collection of dates in  chronological order, $p_t \\in \\mathbb{R}^{24}$ be the day-ahead price indexed by $t \\in \\mathfrak{T}$. Given some historic day-ahead prices $p_{t_1}, p_{t_2}, ... ,p_{t_k}$, forecast the future day-ahead prices $p_{t_{k+1}}, ..., p_{t_{k+h}}$ in a certain future window-size of $h \\in \\mathbb{N}$. In addition, we have a collection of exogenous time series- $d^{\\alpha}_{t_1},d^{\\alpha}_{t_2}, ..., d^{\\alpha}_{t_k}$ indexed by $\\alpha$, which includes the generation and consumption forecasts. Finally for each $t\\in {t_1, ..., t_{k+h'}}, h'>> h$, we have some associated features $f^{\\beta}_t$, where the features are known well into future (for example, if the day is a holiday or not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8j275Z-IjbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractHourlyData(df, data_columns, hour_column='Hour', date_column='Date'):\n",
        "  num_of_sample = len(df['Date'].unique())\n",
        "  num_of_data_columns = len(data_columns)\n",
        "  hourly_data = np.zeros((num_of_sample, 24*num_of_data_columns))\n",
        "  for i, data_column in enumerate(data_columns):\n",
        "    for d in range(24):\n",
        "      hourly_data[:, i*24 + d] = df[df[hour_column] == d].sort_values(by=date_column).loc[:, data_column]\n",
        "  return hourly_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NuEz3EKOTV",
        "colab_type": "text"
      },
      "source": [
        "**Windowed dataset using Keras Timeseries generator**\n",
        "\n",
        "Most models will work not on the entire time-series, but on a windowed subset of the time series. Keras has an inbuilt windowed dataset generator, but it helps to know a few things before we use that. It is primarily meant for use with Keras, so it produces a generator that outputs the dataset in batches as a list $[b_1, b_2, ...]$. Each batch $b_i$ consists of $[X,y]$, with each being a numpy array. The format here is as follows: each $X$ is a windowed dataset with following indices: `(sample_num, timestep, feature)`, while $y$ is of the form `(sample_num, value)`. The assumption here is that `y` consists of a single timestep, which works fine in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIeAIoDKOw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "def get_windowed_dataset(df, X_cols, y_cols, window_size, return_generator=False, **kwargs):\n",
        "  X_data = extractHourlyData(df, X_cols)\n",
        "  y_data = extractHourlyData(df, y_cols)\n",
        "  # use batch size if defined, else return the whole dataset in one batch\n",
        "  batch_size = kwargs['batch_size'] or len(X_data)\n",
        "  generator = TimeseriesGenerator(\n",
        "    X_data, \n",
        "    y_data, \n",
        "    length = window_size, \n",
        "    sampling_rate = 1,\n",
        "    batch_size = batch_size)\n",
        "  if return_generator:\n",
        "    return generator\n",
        "  else:\n",
        "    X_batches = [X for (X, y) in generator]\n",
        "    y_batches = [y for (X, y) in generator]\n",
        "    return X_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrUDKYQGTceQ",
        "colab_type": "text"
      },
      "source": [
        "Just a quick check to see if the generator works as expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4U3qYrTbzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = get_windowed_dataset(\n",
        "    german_df, \n",
        "    ['Day Ahead Price'], \n",
        "    ['Day Ahead Price'], \n",
        "    1, \n",
        "    return_generator=True, \n",
        "    batch_size=len(german_df)\n",
        ")\n",
        "\n",
        "generator[0][0].shape, generator[0][1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9NgiOPfTfxL",
        "colab_type": "text"
      },
      "source": [
        "Before proceeding to investigate our models, it is important to figure out a cross-validation strategy and testing strategy using a metric to assess the performance of each of our models. So, let us briefly touch upon these topics first.  \n",
        "\n",
        "1. The cross-validation strategy for temporal data is a bit different from randomly choosing a subset of the dataset. Since the goal of our analysis is to forecast future values using present data, we will likewise train on a dataset from the past, and validate it against a subset to the future of it. This simulates the actual situation at hand; see [3, 4] for more details.\n",
        "2. For a simple criterion to do a quick comparison of algorithms as a first step in model evaluation, the sMAPE suffices:\n",
        "$$s = \\frac{|y-\\hat{y}|}{2(|y| + |\\hat{y}|)} $$ This should be sufficient for a first-order estimate of how well our models are doing; later on, we can perform more sophisticated forms of tests for our analysis.\n",
        "\n",
        "To cover pt. 1, we create a function to extract dataset into a train and test form chronologically separated as described. We also create a GridsearchCV equivalent for time series for hyperparameter search in ML models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5vGQvcUKmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_train_timesplit(df, date_col_name='Date', train_size=0.9, test_size=0.1):\n",
        "  \"\"\"\n",
        "    Returns test-train split data based on date with test chronologically later than train data.\n",
        "  \"\"\"\n",
        "  min_date = df[date_col_name].min()\n",
        "  max_date = df[date_col_name].max()\n",
        "  train_split_date = min_date + (train_size*(max_date - min_date))\n",
        "  test_split_date = train_split_date + (test_size*(max_date - min_date))\n",
        "  train_df = df[df[date_col_name] < train_split_date]\n",
        "  test_df = df[(df[date_col_name] > train_split_date) & (df[date_col_name] < test_split_date)]\n",
        "  return train_df, test_df\n",
        "\n",
        "def day_forward_chaining(df, date_col_name='Date', k=10):\n",
        "  for i in range(1,k):\n",
        "    yield test_train_timesplit(df, date_col_name, train_size=(i/k), test_size=(1/k))\n",
        "\n",
        "# The above function is the time-series equivalent of gridsearch CV\n",
        "# Example use-case:\n",
        "# for train_df, test_df in day_forward_chaining(german_df):\n",
        "#   <<< do your model training and tuning here >>>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YxvlQ3lry9F",
        "colab_type": "text"
      },
      "source": [
        "For the second part, to compute sMAPE for model evaluation, the following class is defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHq0dSInr4zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class error_analysis:\n",
        "  def __init__(self, predictor):\n",
        "    self.predictor = predictor\n",
        "  def smape_by_entry_from_generator(self, generator):\n",
        "    smape_error = {}\n",
        "    yhat, y = np.zeros((0,24)), np.zeros((0,24))\n",
        "    for X_i, y_i in generator:\n",
        "      yhat_i = self.predictor(X_i)\n",
        "      assert yhat_i.shape == y_i.shape\n",
        "      # add to our list\n",
        "      yhat, y = np.concatenate([yhat_i, yhat], axis=0), np.concatenate([y_i, y], axis=0)\n",
        "    for h in range(24):\n",
        "      smape_error[h] = np.absolute(yhat[:,h] - y[:,h])/(np.absolute(yhat[:,h]) + np.absolute(y[:,h]))\n",
        "    return pd.DataFrame(smape_error)    \n",
        "  def smape_by_hour(self, generator):\n",
        "    error_df = self.smape_by_entry_from_generator(generator)\n",
        "    error_df.replace(np.inf, 0) # remove infinite values\n",
        "    return error_df.mean(axis=0, skipna=True)*200\n",
        "  def total_smape(self, generator):\n",
        "    smape_by_hour = self.smape_by_hour(generator)\n",
        "    return smape_by_hour.mean(axis=0, skipna=True)\n",
        "  def plot(self, generator):\n",
        "    plt.figure(figsize=(15,8))\n",
        "    error_df = self.smape_by_entry(generator)\n",
        "    error_df.boxplot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_afRkMM-Cl",
        "colab_type": "text"
      },
      "source": [
        "# Naive model and the baseline\n",
        "\n",
        "The simplest model one can think of is as follows: $p_{t+h} = p_t $ for all $h$ in the future window. This should allow us to setup a baseline, which any sensible model should perform better than. This is primarily helpful in debugging complex models when making sure that we are not doing anything terribly stupid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAQg6j9NmN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naivemodel(X):\n",
        "  return X[:,0,:]\n",
        "\n",
        "# Since this model needs no training, \n",
        "# we can just test it on the whole dataset\n",
        "X, y = get_windowed_dataset(german_df, ['Day Ahead Price'], ['Day Ahead Price'], 1)\n",
        "\n",
        "def val_generator(): \n",
        "  yield [X, y]\n",
        "\n",
        "naive_error = error_analysis(naivemodel)\n",
        "naive_error.total_smape(val_generator())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8lg9F7UsI5D",
        "colab_type": "text"
      },
      "source": [
        "So, there's our baseline. Anything that performs worse usually means that we are doing something wrong.  \n",
        "\n",
        "Another simple baseline is to simply take the weighted average of the previous values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEAwcicoCa35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mamodel(X, alpha=0.1):\n",
        "  window_size = X.shape[1]\n",
        "  weight = np.array([pow(alpha, i) for i in reversed(range(window_size))])\n",
        "  return np.tensordot(weight, X, axes=(0,1)) / np.sum(weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxKxylBJN9mc",
        "colab_type": "text"
      },
      "source": [
        "# Regression\n",
        "\n",
        "For time-series data, regression usually presents itself in the form of auto-regression, i.e. regression in terms of previous time-step variable. A very simple example would be the $AR(k)$ models, where the hypothesis is as follows:\n",
        "$$p_{t+h}=\\sum_{i=0}\\alpha_ip_{t-i}+\\epsilon$$ where $\\epsilon$ is random white noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUYzrgu_N_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8K_FDnWRyu",
        "colab_type": "text"
      },
      "source": [
        "The above technique is a very simple one; there are far more sophistical statistical techniques available. We will pursue them here through the use of `statsmodels` library. I will not explore the algorithms themselves in detail (though it will make for a good series of blog posts sometime in the near future; for now refer to [1] for details)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8bl3eYi4cx",
        "colab_type": "text"
      },
      "source": [
        "## Univariate AR(k) with time trend\n",
        "\n",
        "A simple way to approach the problem is to consider each hourly day-ahead price as a separate time-series. This lets us use the AR(k) and ARMA(p,q) models on them directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfxAmgJiq4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
        "from statsmodels.tsa.api import acf, pacf, graphics\n",
        "\n",
        "AR_model = AutoReg(german_df, 3, trend='t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeX2O_KrCRsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set_style('darkgrid')\n",
        "sns.mpl.rc('figure',figsize=(16, 6))\n",
        "res.plot_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQoWhaZn6n_",
        "colab_type": "text"
      },
      "source": [
        "## Multivariate: VAR(k) and VECM\n",
        "\n",
        "Instead of treating our series as a univariate time series for each hour, one can expect an improvement by considering it proper as a multivariate time series analysis. The relevant algorithm is the vector autoregressive model (VAR) and the vector error correction model (VECM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U4VJOsxO6Nw",
        "colab_type": "text"
      },
      "source": [
        "# Exponential Smoothing and Holtz-Winter models\n",
        "\n",
        "Another popular approach to time-series problems are the ETS class of techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbcplWTMCD7r",
        "colab_type": "text"
      },
      "source": [
        "$$y_t = y_{t-1} + \\alpha (y_{t-1} - \\hat{y}_{t-1})$$\n",
        "\n",
        "$$y_{t+h} = l_t +h b_t + s_{t+h-m(k+1)}$$\n",
        "$$l_t = \\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1} + b_{t-1})$$\n",
        "$$b_t = \\beta^*(l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}$$\n",
        "$$s_t = \\gamma \\frac{y_t}{(l_{t-1} + b_{t-1})} + (1-\\gamma)s_{t-m}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWw8aLSsO9b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4twMe_WHm0",
        "colab_type": "text"
      },
      "source": [
        "# Markov switching regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDQ3w4wWNXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woxv6ti6kMbM",
        "colab_type": "text"
      },
      "source": [
        "# Bibliography\n",
        "\n",
        "**[HA18]** R. J. Hyndman, and G. Athanasopoulos, Forecasting: Principles and Practice, https://otexts.com/fpp2/index.html \n",
        "\n",
        "**[HWL15]**. R. J. Hyndman, E. Wang, N. Laptev, *Large-Scale Unusual Time Series Detection*,  [https://robjhyndman.com/papers/icdm2015.pdf]\n",
        "\n",
        "**[SE11]** More on the different approaches to the validation strategies for a time series here: https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection\n",
        "\n",
        "**[Tas00]** L. J. Tashman, *Out-of-sample tests of forecasting accuracy: an analysis and review*, International Journal of Forecasting, 2000, vol. 16, issue 4, 437-450 [https://econpapers.repec.org/article/eeeintfor/v_3a16_3ay_3a2000_3ai_3a4_3ap_3a437-450.htm]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1CXDz1x7GX",
        "colab_type": "text"
      },
      "source": [
        "Supplements:  \n",
        "[Using Facebook's forecast engine: Prophet]()  \n",
        "[GluonTS]()\n",
        "\n",
        "Next part: [Deep Models]()"
      ]
    }
  ]
}